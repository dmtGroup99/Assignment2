load("lda_user.rda")
?dist
jaccard.index(1:10, 2:20)
library(text2vec)
sim2(1:10,2:20, method = "cosine")
B = matrix(
c(2, 4, 3, 1, 5, 7),
nrow=3,
ncol=2)
C <- t(B)
sim2(B,C, method = "cosine")
x<-rnorm(10)
x<-matrix(x)
y<-matrix(rnorm(10*5),nrow=5)
x
y
install.packages("shiny")
install.packages("Shines")
install.packages("shinyjs")
install.packages("DT")
library(data.table)
data <- data.table::fread(input = "../dataset_mood_smartphone.csv",drop = 1)
ndcg <- function(label, id, predictions, k){
d <- data.frame(relevance = label, id=id, order = predictions, one=1)
d <- d[order(d$id, -d$relevance),]
d$co<-with(d, unlist(tapply(one, id, cumsum)))
d$v<-with(d, (2^relevance-1)/(log(co+1, 2)))
dcg0<-with(d[d$co<=k,], unlist(tapply(v, id, sum)))
d<-d[order(d$id, -d$order),]
d$co<-with(d, unlist(tapply(one, id, cumsum)))
d$v<-with(d, (2^relevance-1)/(log(co+1, 2)))
dcg<-with(d[d$co<=k,], unlist(tapply(v, id, sum)))
return(mean(dcg/dcg0))
}
predictions <- c(0.9, 0.7, 0.6,0.8,0.4,0,0.5,0 )
id <- c(1:8)
ndcg(label, id, predictions, 6)
label <- c(3,3,3,2,2,2,1,0)
ndcg(label, id, predictions, 6)
k=6
d <- data.frame(relevance = label, id=id, order = predictions, one=1)
d
id <- c(1,1,1,1,1,1,1,1)
d <- data.frame(relevance = label, id=id, order = predictions, one=1)
d
d <- d[order(d$id, -d$relevance),]
d
d$co<-with(d, unlist(tapply(one, id, cumsum)))
d$v<-with(d, (2^relevance-1)/(log(co+1, 2)))
dcg0<-with(d[d$co<=k,], unlist(tapply(v, id, sum)))
dcg0
d<-d[order(d$id, -d$order),]
d$co<-with(d, unlist(tapply(one, id, cumsum)))
d$v<-with(d, (2^relevance-1)/(log(co+1, 2)))
dcg<-with(d[d$co<=k,], unlist(tapply(v, id, sum)))
dcg
mean(dcg/dcg0)
d
d <- data.frame(relevance = label, id=id, order = predictions, one=1)
d
d <- d[order(d$id, -d$relevance),]
d$co<-with(d, unlist(tapply(one, id, cumsum)))
d
d$v<-with(d, (2^relevance-1)/(log(co+1, 2)))
d
2^(3)-1
log(2,2)
2^(3)-1
log(3,2)
7/log(3,2)
dcg0<-with(d[d$co<=k,], unlist(tapply(v, id, sum)))
d<-d[order(d$id, -d$order),]
d$co<-with(d, unlist(tapply(one, id, cumsum)))
d$v<-with(d, (2^relevance-1)/(log(co+1, 2)))
d
predictions <- c(0.9, 0, 0.6,0.8,0.3,0,0.4,0.5 )
d <- data.frame(relevance = label, id=id, order = predictions, one=1)
d <- d[order(d$id, -d$relevance),]
d$co<-with(d, unlist(tapply(one, id, cumsum)))
d$v<-with(d, (2^relevance-1)/(log(co+1, 2)))
dcg0<-with(d[d$co<=k,], unlist(tapply(v, id, sum)))
d<-d[order(d$id, -d$order),]
d$co<-with(d, unlist(tapply(one, id, cumsum)))
d$v<-with(d, (2^relevance-1)/(log(co+1, 2)))
d
dcg<-with(d[d$co<=k,], unlist(tapply(v, id, sum)))
mean(dcg/dcg0)
setwd("~/Desktop/DT/dat_kick-off")
library(data.table)
dr3 <- data.table::fread(input = "flat_dr3.csv", data.table = FALSE)
dr3 <- dr3[,-which(names(dr3) %in% c('index', 'type', 'id', 'score', 'ebinumber', 'kvknumber', 'vestnumber',
'street', 'zipcode', 'city','longitude', 'latitude','namepartone', 'mainsbi', 'starter', 'sbi1',
'nace', 'employeewhitecollarclass',
'relationshiptype', 'municipalitysizecode',
'egg','municipalitysize',
'revenueclass', 'employeetotalwhitecollar',
'bedstotal', 'residentialunitstotal', 'premises', 'revenue', 'needcomputersupplies',
'needofficesupplies'))]
dr3 <- dr3[!duplicated(dr3$customerid),]
View(dr3)
yellow <- data.table::fread(input = "flat_yellowpages.csv", data.table = FALSE)
yellow <- yellow[,c("customerid", "paid", "category", "city")]
yellow <- yellow[!duplicated(yellow$customerid),]
colnames(yellow) <- c("customerid", "paid", "categoryname", "locationname")
View(yellow)
View(yellow)
dr3 <- data.table::fread(input = "flat_dr3.csv", data.table = FALSE)
dr3 <- dr3[,-which(names(dr3) %in% c('index', 'type', 'id', 'score', 'ebinumber', 'kvknumber', 'vestnumber',
'street', 'zipcode', 'city','longitude', 'latitude','namepartone', 'mainsbi', 'starter', 'sbi1',
'nace', 'employeewhitecollarclass',
'relationshiptype', 'municipalitysizecode',
'egg','municipalitysize',
'revenueclass', 'employeetotalwhitecollar',
'bedstotal', 'residentialunitstotal', 'premises', 'revenue', 'needcomputersupplies',
'needofficesupplies'))]
dr3 <- dr3[!duplicated(dr3$customerid),]
yellow <- data.table::fread(input = "flat_yellowpages.csv", data.table = FALSE)
yellow <- yellow[,c("customerid", "paid", "category", "city")]
yellow <- yellow[!duplicated(yellow$customerid),]
colnames(yellow) <- c("customerid", "paid", "categoryname", "locationname")
dat <- merge(dr3,yellow,by="customerid")
dat$categoryname <- gsub("\\[|\\]", "", dat$categoryname)
dat$locationname <- gsub("\\[|\\]", "", dat$locationname)
dat_nonpaid <- dat[dat$paid=="false",]
View(dat_nonpaid)
unique(dat_nonpaid$paid)
length(dat[,1]) - length(dat_nonpaid[,1])
dr3 <- data.table::fread(input = "flat_dr3.csv", data.table = FALSE)
dr3 <- dr3[,-which(names(dr3) %in% c('index', 'type', 'id', 'score', 'ebinumber', 'kvknumber', 'vestnumber',
'street', 'zipcode', 'city','longitude', 'latitude','namepartone', 'mainsbi', 'starter', 'sbi1',
'nace', 'employeewhitecollarclass',
'relationshiptype', 'municipalitysizecode',
'egg','municipalitysize',
'revenueclass', 'employeetotalwhitecollar',
'bedstotal', 'residentialunitstotal', 'premises', 'revenue', 'needcomputersupplies',
'needofficesupplies'))]
aa <- dr3[duplicated(dr3$customerid),]
yellow <- data.table::fread(input = "flat_yellowpages.csv", data.table = FALSE)
yellow <- yellow[,c("customerid", "paid", "category", "city")]
dupl <- yellow[duplicated(yellow$customerid),]
View(dupl)
View(yellow)
a <- yellow[yellow$customerid==1243753, ]
a
a[!duplicated(a$customerid),]
yellow <- yellow[!(duplicated(yellow$customerid) | duplicated(yellow$customerid, fromLast = TRUE)), ]
yellow_out_dupli <- yellow
yellow <- data.table::fread(input = "flat_yellowpages.csv", data.table = FALSE)
yellow <- yellow[,c("customerid", "paid", "category", "city")]
yellow <- yellow[!duplicated(yellow$customerid),]
colnames(yellow) <- c("customerid", "paid", "categoryname", "locationname")
length(yellow[,1]) - length(yellow_out_dupli[,1])
View(yellow_out_dupli)
dr3 <- data.table::fread(input = "flat_dr3.csv", data.table = FALSE)
dr3 <- dr3[,-which(names(dr3) %in% c('index', 'type', 'id', 'score', 'ebinumber', 'kvknumber', 'vestnumber',
'street', 'zipcode', 'city','longitude', 'latitude','namepartone', 'mainsbi', 'starter', 'sbi1',
'nace', 'employeewhitecollarclass',
'relationshiptype', 'municipalitysizecode',
'egg','municipalitysize',
'revenueclass', 'employeetotalwhitecollar',
'bedstotal', 'residentialunitstotal', 'premises', 'revenue', 'needcomputersupplies',
'needofficesupplies'))]
dr3 <- dr3[!duplicated(dr3$customerid),]
yellow <- data.table::fread(input = "flat_yellowpages.csv", data.table = FALSE)
yellow <- yellow[,c("customerid", "paid", "category", "city")]
#yellow <- yellow[!duplicated(yellow$customerid),]
#Remove all duplicates. If there is a duplicate. Removes all customerids of that duplicate
yellow <- yellow[!(duplicated(yellow$customerid) | duplicated(yellow$customerid, fromLast = TRUE)), ]
colnames(yellow) <- c("customerid", "paid", "categoryname", "locationname")
dat <- merge(dr3,yellow,by="customerid")
dat$categoryname <- gsub("\\[|\\]", "", dat$categoryname)
dat$locationname <- gsub("\\[|\\]", "", dat$locationname)
dat_nonpaid <- dat[dat$paid=="false",]
write.csv(dat_nonpaid, "dat_nonpaid_20180509_withoutDuplicates.csv" )
1135478-1092744
42734/1135478
setwd("~/Desktop/Assignment2")
library(data.table)
data <- data.table::fread(input = "../training_set_VU_DM_2014.csv", na.strings = c("NA", "NULL", "null"), data.table = FALSE)
missing_percentage <- apply(data, 2, function(col) sum(is.na(col))/length(col))
#This will give 34 variables
keep <- names(missing_percentage[missing_percentage<0.8])
data_ver2 <- subset(data, select = keep)
keep_id <- unique(data_ver2$srch_id)
#split data random into train and test
dt <- sample(keep_id, train_instances*length(keep_id))
train <- subset(data_ver2, srch_id %in% dt)
test <- subset(data_ver2, !(srch_id %in% dt))
train_instances <- 0.8
missing_percentage <- apply(data, 2, function(col) sum(is.na(col))/length(col))
#This will give 34 variables
keep <- names(missing_percentage[missing_percentage<0.8])
data_ver2 <- subset(data, select = keep)
keep_id <- unique(data_ver2$srch_id)
#split data random into train and test
dt <- sample(keep_id, train_instances*length(keep_id))
train <- subset(data_ver2, srch_id %in% dt)
test <- subset(data_ver2, !(srch_id %in% dt))
aa <- train[,c("prop_review_score")]
head(aa)
head(aa[is.na(aa)])
k <- aa[is.na(aa)]
k
is.na(aa)
head(numeric(is.na(aa)))
as.numeric(is.na(aa))
q <- as.numeric(is.na(aa))
unique(q)
k <- aa[is.na(aa)] <- 0
k
aa[is.na(aa)] <- 0
View(aa)
num_review <- train[,c("prop_review_score")]
num_review$binary_review <- as.numeric(is.na(num_review))
num_review[is.na(num_review)] <- 0
head(num_review)
num_review <- train[,c("prop_review_score")]
a <- data.frame(num_review = num_review, binary_review = as.numeric(is.na(num_review)))
head(a)
num_review <- train[,c("prop_review_score")]
binary_review <- as.numeric(is.na(num_review))
num_review[is.na(num_review)] <- 0
review <- data.frame(num_review = num_review, binary_review = binary_review)
data <- data.table::fread(input = "../training_set_VU_DM_2014.csv", na.strings = c("NA", "NULL", "null"), data.table = FALSE)
source("data_scripts_v2/iteration6_v2.R")
setwd("~/Desktop/Assignment2/Version2")
source("data_scripts_v2/iteration6_v2.R")
data_new <- iteration6(data, 0.8)
setwd("~/Desktop/Assignment2/Version2/data_scripts_v2")
setwd("~/Desktop/Assignment2/Version2/iterations")
setwd("~/Desktop/Assignment2")
library(data.table)
data <- data.table::fread(input = "../training_set_VU_DM_2014.csv", na.strings = c("NA", "NULL", "null"), data.table = FALSE)
source("Version2/data_scripts_v2/iteration9_v2.R")
data_new <- iteration9(data, 0.8)
data_new <- iteration9(data, 0.8)
missing_percentage <- apply(data, 2, function(col) sum(is.na(col))/length(col))
#This will give 34 variables
keep <- names(missing_percentage[missing_percentage<0.8])
data_ver2 <- subset(data, select = keep)
keep_id <- unique(data_ver2$srch_id)
#split data random into train and test
dt <- sample(keep_id, train_instances*length(keep_id))
train <- subset(data_ver2, srch_id %in% dt)
test <- subset(data_ver2, !(srch_id %in% dt))
#split data into numerical and nominal values (check this!)
num <- train[,c("prop_location_score1","prop_location_score2", "prop_log_historical_price","price_usd",
"srch_length_of_stay","srch_booking_window","srch_adults_count","srch_children_count",
"srch_room_count","orig_destination_distance", "position")]
# replace na values in review with zero, because this can be seen as something negative
# also add a binary attribute that says if value is missing from review
num_review <- train[,c("prop_review_score")]
binary_review <- as.numeric(is.na(num_review))
num_review[is.na(num_review)] <- 0
review <- data.frame(prop_review_score = num_review, binary_review = binary_review)
nom <- train[,c("srch_id", "site_id", "visitor_location_country_id", "prop_country_id", "prop_id", "prop_starrating",
"prop_brand_bool", "promotion_flag", "srch_destination_id", "srch_saturday_night_bool", "random_bool",
"comp2_rate", "comp2_inv", "comp3_rate", "comp3_inv", "comp5_rate", "comp5_inv", "comp8_rate", "comp8_inv")]
num_median <- apply(num, 2, function(x) median(x, na.rm = TRUE))
nom_occur <- apply(nom, 2, function(x) names(which.max(table(x))))
numeric_median <- function(x, num_median){
for(i in 1:length(x[1,])){
x[,i][is.na(x[,i])] <- unname(num_median[i])
}
return(x)
}
nominal_occurence <- function(x, nom_occur){
for(i in 1:length(x[1,])){
x[,i][is.na(x[,i])] <- unname(nom_occur[i])
}
return(x)
}
num <- numeric_median(num, num_median)
nom <- nominal_occurence(nom, nom_occur)
#make numeric
nom <- as.data.frame(lapply(nom, function(x) as.numeric(as.character(x))))
#give relevance scores (clicked -> 1, booked -> 5 and other -> 0)
target <- train$booking_bool + train$click_bool
target[target==2] <- 5
train_set <- cbind(num, nom, review, target)
positionmedian <- with(train_set, tapply(position, prop_id, function(x) median(x)))
key <- data.frame(median_postition = unname(positionmedian), id = names(positionmedian))
train_set$estimate_position <- key[match(train_set$prop_id, key$id), "median_postition"]
train_set$position <- NULL
#feature engineering
#calculating differences with respect to mean of search query numerical values
train_set$propscore1diff <- with(train_set, unlist(tapply(prop_location_score1, srch_id, function(x) x-median(x))))
train_set$propscore2diff <- with(train_set, unlist(tapply(prop_location_score2, srch_id, function(x) x-median(x))))
train_set$propreviewscorediff <- with(train_set, unlist(tapply(prop_review_score, srch_id, function(x) x-median(x))))
train_set$propstarratingdiff <- with(train_set, unlist(tapply(prop_starrating, srch_id, function(x) x-median(x))))
train_set$priceusddiff <- with(train_set, unlist(tapply(price_usd, srch_id, function(x) x-median(x))))
#split data into numerical and nominal values (check this!)
num <- test[,c("prop_location_score1","prop_location_score2", "prop_log_historical_price","price_usd",
"srch_length_of_stay","srch_booking_window","srch_adults_count","srch_children_count",
"srch_room_count","orig_destination_distance")]
# replace na values in review with zero, because this can be seen as something negative
# also add a binary attribute that says if value is missing from review
num_review <- test[,c("prop_review_score")]
binary_review <- as.numeric(is.na(num_review))
num_review[is.na(num_review)] <- 0
review <- data.frame(prop_review_score = num_review, binary_review = binary_review)
nom <- test[,c("srch_id", "site_id", "visitor_location_country_id", "prop_country_id", "prop_id", "prop_starrating",
"prop_brand_bool", "promotion_flag", "srch_destination_id", "srch_saturday_night_bool", "random_bool",
"comp2_rate", "comp2_inv", "comp3_rate", "comp3_inv", "comp5_rate", "comp5_inv", "comp8_rate", "comp8_inv")]
num <- numeric_median(num, num_median)
nom <- nominal_occurence(nom, nom_occur)
#make numeric
nom <- as.data.frame(lapply(nom, function(x) as.numeric(as.character(x))))
#give relevance scores (clicked -> 1, booked -> 5 and other -> 0)
target <- test$booking_bool + test$click_bool
target[target==2] <- 5
test_set <- cbind(num, nom, review, target)
d <- subset(test_set, !(test_set$prop_id %in% train_set$prop_id))
key2 <- data.frame(median_postition = median(positionmedian), id = as.factor(unique(d$prop_id)))
key_final <- rbind(key,key2)
#key_final <- subset(key_final, test_set$prop_id %in% key_final$id)
test_set$estimate_position <- key_final[match(test_set$prop_id, key_final$id), "median_postition"]
test_set$position <- NULL
test_set$propscore1diff <- with(test_set, unlist(tapply(prop_location_score1, srch_id, function(x) x-median(x))))
test_set$propscore2diff <- with(test_set, unlist(tapply(prop_location_score2, srch_id, function(x) x-median(x))))
test_set$propreviewscorediff <- with(test_set, unlist(tapply(prop_review_score, srch_id, function(x) x-median(x))))
test_set$propstarratingdiff <- with(test_set, unlist(tapply(prop_starrating, srch_id, function(x) x-median(x))))
test_set$priceusddiff <- with(test_set, unlist(tapply(price_usd, srch_id, function(x) x-median(x))))
out <- list()
out$train <- train_set
out$test <- test_set
colnames(train)
colnames(train_set)
data_new <- out
library(xgboost)
train <- data_new$train[order(data_new$train$srch_id),]
counts <- data.frame(table(train$srch_id))
colnames(test_set)
dtrain <- xgb.DMatrix(as.matrix(train[,-c(11,32)]), label = train$target, group = counts$Freq)
dtest <- xgb.DMatrix(as.matrix(data_new$test[,-c(11,32)]))
bst <- xgb.train(data = dtrain, max.depth = 5, eta = 1, nthread = 5, nround = 100, objective = "rank:pairwise",
eval_metric = "ndcg", verbose = 3)
importance_matrix <- xgb.importance(colnames(dtrain),model = bst)
print(importance_matrix)
xgb.plot.importance(importance_matrix = importance_matrix)
predictions <- predict(bst,dtest)
id <- data_new$test$srch_id
label <- data_new$test$target
source("ndcg_function.R")
source("Version2/ndcg_function.R")
metric <- ndcg(label, id, predictions, 38)
metric
setwd("~/Desktop/Assignment2/Version2/data_scripts_v2")
setwd("~/Desktop/Assignment2/Version2/iterations")
setwd("~/Desktop/Assignment2")
library(data.table)
#load data
data <- data.table::fread(input = "../training_set_VU_DM_2014.csv", na.strings = c("NA", "NULL", "null"), data.table = FALSE)
#preprocess data
source("Version2/data_scripts_v2/iteration10_v2.R")
data_new <- iteration10(data, 0.8)
source("Version2/data_scripts_v2/iteration10_v2.R")
source("Version2/data_scripts_v2/iteration9_v2.R")
data_new <- iteration9(data, 0.8)
library(xgboost)
train <- data_new$train[order(data_new$train$srch_id),]
counts <- data.frame(table(train$srch_id))
colnames(train)
dtrain <- xgb.DMatrix(as.matrix(train[,-c(11,32)]), label = train$target, group = counts$Freq)
dtest <- xgb.DMatrix(as.matrix(data_new$test[,-c(11,32)]))
bst <- xgb.train(data = dtrain, max.depth = 5, eta = 1, nthread = 5, nround = 100, objective = "rank:pairwise",
eval_metric = "ndcg", verbose = 3)
importance_matrix <- xgb.importance(colnames(dtrain),model = bst)
print(importance_matrix)
xgb.plot.importance(importance_matrix = importance_matrix)
colnames(train)
source("Version2/data_scripts_v2/iteration10_v2.R")
View(iteration10)
View(iteration10)
data_new <- iteration10(data, 0.8)
missing_percentage <- apply(data, 2, function(col) sum(is.na(col))/length(col))
missing_percentage <- apply(data, 2, function(col) sum(is.na(col))/length(col))
#This will give 34 variables
keep <- names(missing_percentage[missing_percentage<0.8])
data_ver2 <- subset(data, select = keep)
keep_id <- unique(data_ver2$srch_id)
#split data random into train and test
dt <- sample(keep_id, train_instances*length(keep_id))
train <- subset(data_ver2, srch_id %in% dt)
test <- subset(data_ver2, !(srch_id %in% dt))
#split data into numerical and nominal values (check this!)
num <- train[,c("prop_location_score1","prop_location_score2", "prop_log_historical_price","price_usd",
"srch_length_of_stay","srch_booking_window","srch_adults_count","srch_children_count",
"srch_room_count","orig_destination_distance", "position")]
# replace na values in review with zero, because this can be seen as something negative
# also add a binary attribute that says if value is missing from review
num_review <- train[,c("prop_review_score")]
binary_review <- as.numeric(is.na(num_review))
num_review[is.na(num_review)] <- 0
review <- data.frame(prop_review_score = num_review, binary_review = binary_review)
nom <- train[,c("srch_id", "site_id", "visitor_location_country_id", "prop_country_id", "prop_id", "prop_starrating",
"prop_brand_bool", "promotion_flag", "srch_destination_id", "srch_saturday_night_bool", "random_bool",
"comp2_rate", "comp2_inv", "comp3_rate", "comp3_inv", "comp5_rate", "comp5_inv", "comp8_rate", "comp8_inv")]
num_median <- apply(num, 2, function(x) median(x, na.rm = TRUE))
nom_occur <- apply(nom, 2, function(x) names(which.max(table(x))))
numeric_median <- function(x, num_median){
for(i in 1:length(x[1,])){
x[,i][is.na(x[,i])] <- unname(num_median[i])
}
return(x)
}
nominal_occurence <- function(x, nom_occur){
for(i in 1:length(x[1,])){
x[,i][is.na(x[,i])] <- unname(nom_occur[i])
}
return(x)
}
num <- numeric_median(num, num_median)
nom <- nominal_occurence(nom, nom_occur)
#make numeric
nom <- as.data.frame(lapply(nom, function(x) as.numeric(as.character(x))))
#give relevance scores (clicked -> 1, booked -> 5 and other -> 0)
target <- train$booking_bool + train$click_bool
target[target==2] <- 5
train_set <- cbind(num, nom, review, target)
positionmedian <- with(train_set, tapply(position, prop_id, function(x) median(x)))
key <- data.frame(median_postition = unname(positionmedian), id = names(positionmedian))
train_set$estimate_position <- key[match(train_set$prop_id, key$id), "median_postition"]
train_set$position <- NULL
train_set$binary_review <- NULL
#feature engineering
#calculating differences with respect to mean of search query numerical values
train_set$propscore1diff <- with(train_set, unlist(tapply(prop_location_score1, srch_id, function(x) x-median(x))))
train_set$propscore2diff <- with(train_set, unlist(tapply(prop_location_score2, srch_id, function(x) x-median(x))))
train_set$propreviewscorediff <- with(train_set, unlist(tapply(prop_review_score, srch_id, function(x) x-median(x))))
train_set$propstarratingdiff <- with(train_set, unlist(tapply(prop_starrating, srch_id, function(x) x-median(x))))
train_set$priceusddiff <- with(train_set, unlist(tapply(price_usd, srch_id, function(x) x-median(x))))
train_set$meanreview <- with(train_set, unlist(tapply(prop_review_score, srch_id, function(x) mean(x) - x))) + train_set$prop_review_score
#split data into numerical and nominal values (check this!)
num <- test[,c("prop_location_score1","prop_location_score2", "prop_log_historical_price","price_usd",
"srch_length_of_stay","srch_booking_window","srch_adults_count","srch_children_count",
"srch_room_count","orig_destination_distance")]
# replace na values in review with zero, because this can be seen as something negative
# also add a binary attribute that says if value is missing from review
num_review <- test[,c("prop_review_score")]
binary_review <- as.numeric(is.na(num_review))
num_review[is.na(num_review)] <- 0
review <- data.frame(prop_review_score = num_review, binary_review = binary_review)
nom <- test[,c("srch_id", "site_id", "visitor_location_country_id", "prop_country_id", "prop_id", "prop_starrating",
"prop_brand_bool", "promotion_flag", "srch_destination_id", "srch_saturday_night_bool", "random_bool",
"comp2_rate", "comp2_inv", "comp3_rate", "comp3_inv", "comp5_rate", "comp5_inv", "comp8_rate", "comp8_inv")]
num <- numeric_median(num, num_median)
nom <- nominal_occurence(nom, nom_occur)
#make numeric
nom <- as.data.frame(lapply(nom, function(x) as.numeric(as.character(x))))
#give relevance scores (clicked -> 1, booked -> 5 and other -> 0)
target <- test$booking_bool + test$click_bool
target[target==2] <- 5
test_set <- cbind(num, nom, review, target)
d <- subset(test_set, !(test_set$prop_id %in% train_set$prop_id))
key2 <- data.frame(median_postition = median(positionmedian), id = as.factor(unique(d$prop_id)))
key_final <- rbind(key,key2)
#key_final <- subset(key_final, test_set$prop_id %in% key_final$id)
test_set$estimate_position <- key_final[match(test_set$prop_id, key_final$id), "median_postition"]
test_set$position <- NULL
test_set$binary_review <- NULL
test_set$propscore1diff <- with(test_set, unlist(tapply(prop_location_score1, srch_id, function(x) x-median(x))))
test_set$propscore2diff <- with(test_set, unlist(tapply(prop_location_score2, srch_id, function(x) x-median(x))))
test_set$propreviewscorediff <- with(test_set, unlist(tapply(prop_review_score, srch_id, function(x) x-median(x))))
test_set$propstarratingdiff <- with(test_set, unlist(tapply(prop_starrating, srch_id, function(x) x-median(x))))
test_set$priceusddiff <- with(test_set, unlist(tapply(price_usd, srch_id, function(x) x-median(x))))
test_set$meanreview <- with(test_set, unlist(tapply(prop_review_score, srch_id, function(x) mean(x) - x))) + test_set$prop_review_score
out <- list()
out$train <- train_set
out$test <- test_set
data_new <- out
train <- data_new$train[order(data_new$train$srch_id),]
train$meanreview
counts <- data.frame(table(train$srch_id))
colnames(train)
dtrain <- xgb.DMatrix(as.matrix(train[,-c(11,31)]), label = train$target, group = counts$Freq)
dtest <- xgb.DMatrix(as.matrix(data_new$test[,-c(11,32)]))
bst <- xgb.train(data = dtrain, max.depth = 5, eta = 1, nthread = 6, nround = 100, objective = "rank:pairwise",
eval_metric = "ndcg", verbose = 3)
importance_matrix <- xgb.importance(colnames(dtrain),model = bst)
print(importance_matrix)
xgb.plot.importance(importance_matrix = importance_matrix)
predictions <- predict(bst,dtest)
id <- data_new$test$srch_id
label <- data_new$test$target
source("Version2/ndcg_function.R")
metric <- ndcg(label, id, predictions, 38)
metric <- ndcg(label, id, predictions, 38)
metric
data <- data.table::fread(input = "../training_set_VU_DM_2014.csv", na.strings = c("NA", "NULL", "null"), data.table = FALSE)
#preprocess data
source("Version2/data_scripts_v2/iteration10_v2.R")
data_new <- iteration10(data, 0.8)
library(xgboost)
train <- data_new$train[order(data_new$train$srch_id),]
counts <- data.frame(table(train$srch_id))
colnames(train)
dtrain <- xgb.DMatrix(as.matrix(train[,-c(11,31)]), label = train$target, group = counts$Freq)
dtest <- xgb.DMatrix(as.matrix(data_new$test[,-c(11,32)]))
bst <- xgb.train(data = dtrain, max.depth = 5, eta = 1, nthread = 6, nround = 100, objective = "rank:pairwise",
eval_metric = "ndcg", verbose = 3)
importance_matrix <- xgb.importance(colnames(dtrain),model = bst)
print(importance_matrix)
xgb.plot.importance(importance_matrix = importance_matrix)
predictions <- predict(bst,dtest)
id <- data_new$test$srch_id
label <- data_new$test$target
source("Version2/ndcg_function.R")
metric <- ndcg(label, id, predictions, 38)
metric
colnames(data_new$test)
colnames(train)

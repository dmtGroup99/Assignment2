library(nycflights13)
nycflights13::flights
flights
flights <- flights
colnames(flights)
View(flights)
summarise(flights)
?summarise
filter(flights, month==1, day==1)
head(mpg)
a <- filter(mpg, cyl<=4)
a
View(a)
a <- filter(mpg, class == "compact", drv == "f")
jan1 <- filter(flights, month == 1, day == 1)
head(jan1)
(dec25 <- filter(flights, month == 12, day == 25))
filter(flights, month = 1)
sqrt(2)^2
sqrt(2)^2 == 2
?near
near(sqrt(2)^2, 2)
filter(flights, month == 11, month == 12)
filter(flights, month == 11 | month == 12)
filter(flights, !(month == 11 | month == 12))
a <- filter(flights, month == 11 | month == 12)
nov_dec <- filter(flights, month %in% c(11,12))
View(nov_dec)
?is
x <- 3
is.na(x)
df <- tibble(x = c(1, NA, 3))
head(df)
df
filter(df,x>1)
filter(df, is.na(x) | x>1)
colnames(flights)
( a <- filter(flights, arr_delay >= 120 )
)
(a <- filter(flights, dest = "IAH" | dest = "HOU"))
(a <- filter(flights, dest == "IAH" | dest == "HOU"))
(a <- filter(flights, month == 7 | month == 8 | month == 9))
colnames(flights)
(a <- filter(flights, arr_time > 120 & sched_dep_time == dep_time))
View(a)
a <- flights
View(a)
2354-2251
?between
(a <- filter(flights, between(month, 4, 10)) )
View(a)
(a <- filter(flights, is.na(dep_time)) )
View(a)
NA^0
NA|TRUE
FALSE & NA
NA * 0
TRUE & NA
(a <- arrange(flights, year, month, day))
View(a)
(a <- arrange(flights, desc(arr_delay)))
(a <- arrange(flights, desc(arr_delay, sched_arr_time)))
(a <- arrange(flights, desc(sched_arr_time, arr_delay)))
df <- tibble(x = c(5, 2, NA))
df
arrange(df, x)
arrange(df, desc(x))
arrange(df, is.na(x))
arrange(df, desc(is.na(x)))
flights_sml <- select(flights,
year:day,
ends_with("delay"),
distance,
air_time
)
View(flights_sml)
library(tydiverse)
library(tidyverse)
library(nycflights13)
flights <- flights
View(flights)
flights20 <- select(flights, year:flight, gain = arr_de )
flights20 <- select(flights, year:flight, gain = arr_delay - dep_delay, speed = distance/air_time*60 )
colnumes(flights)
colnames(fligths)
colnames(flights)
flights20 <- select(flights, year:flight, gain = arr_delay - dep_delay, speed = distance / air_time * 60 )
flights20 <- mutate(gain = arr_delay - dep_delay, speed = distance / air_time * 60 )
flights20 <- mutate(flights, gain = arr_delay - dep_delay, speed = distance / air_time * 60 )
View(flights20)
head(flights20)
a <- transmute(flights, dep_hour = dep_time %/% 60)
head(a)
8*60
a <- transmute(flights, dep_hour = dep_time %/% 60, dep_minute = dep_time %% 60)
head(a)
b <- mutate(a, dep_time = dep_hour * 60 + dep_minute)
head(b)
?flights
x <- seq(1,10)
(x <- seq(1,10))
cumsum(x)
cumsum(x)
cumprod(x)
y <- c(1,2,2,NA,3,4)
min_rank(y)
?min_rank
transmute(flight, new = air_time - dep_time)
transmute(flights, new = air_time - dep_time)
1:3
1:3 + 1:10
summarise(flights, delya = mean(dep_delay, na.rm = TRUE))
summarise(flights)
by_dest <- group_by(flights, dest)
by_dest
by_dest <- group_by(flights, dest)
delay <- summarise(by_dest,
count = n(),
dist = mean(distance, na.rm = TRUE),
delay = mean(arr_delay, na.rm = TRUE)
)
delay <- filter(delay, count > 20, dest != "HNL")
delay
by_dest <- group_by(flights, dest)
delay <- summarise(by_dest,
count = n(),
dist = mean(distance, na.rm = TRUE),
delay = mean(arr_delay, na.rm = TRUE)
)
delay
?n()
flights %/% group_by(year, month, day) %/% summarise(mean = mean(dep_delay))
flights %>% group_by(year, month, day) %>% summarise(mean = mean(dep_delay))
flights %>% group_by(year, month, day) %>% summarise(mean = mean(dep_delay, na.rm = T))
not_cancelled <- flights %>% filter(!is.na(dep_delay), !is.na(arr_delay)) %>% summarise(mean = mean(dep_delay))
not_cancelled
not_cancelled <- flights %>% filter(!is.na(dep_delay), !is.na(arr_delay)) %>% group_by(year, month, day) %>% summarise(mean = mean(dep_delay))
not_cancelled
group_by(not_cancelled, tailnum)
not_cancelled %>% group_by(tailnum)
not_cancelled <- flights %>%
filter(!is.na(dep_delay), !is.na(arr_delay))
not_cancelled %>%
group_by(year, month, day) %>%
summarise(mean = mean(dep_delay))
delays <- not_cancelled %>%
group_by(tailnum) %>%
summarise(
delay = mean(arr_delay)
)
delays
not_cancelled
not_cancelled <- flights %>% filter(!is.na(dep_delay), !is.na(arr_delay)) %>% group_by(year, month, day) %>% summarise(mean = mean(dep_delay))
not_cancelled
not_cancelled <- flights %>%
filter(!is.na(dep_delay), !is.na(arr_delay))
not_cancelled %>%
group_by(year, month, day) %>%
summarise(mean = mean(dep_delay))
not_cancelled %>% group_by(tailnum)
not_cancelled
not_cancelled %>%
group_by(year, month, day) %>%
summarise(hour_perc = mean(arr_delay > 60))
not_cancelled
?glimpse
not_cancelled %>%
group_by(year, month, day) %>%
summarise(mean = mean(dep_delay))
not_cancelled <- flights %>%
filter(!is.na(dep_delay), !is.na(arr_delay))
source('~/visualisation.R')
ggplot(data = mpg) + geom_point(mapping = aes(x = displ , y = hwy ))
source('~/visualisation.R')
library(tidyverse)
dat  <-  gapminder
library(gapminder)
install.packages("gapminder")
library(gapminder)
dat <- gapminder
View(gapminder)
?gapminder
head(gapminder)
typeof(gapminder)
library(tidyverse)
?rnorm
df <- tibble(x = rnorm(100,3,5), y = rnorm(100,10,5))
df
df <-tibble( x = rnorm(100,3,5), y = rnorm(100,10,5))
(df <-tibble( x = rnorm(100,3,5), y = rnorm(100,10,5)))
colnum(df)
?col_number
col_number(df)
dim(df)
?seq
rep(1,10)
rep(1,10) + zeros(1,10)
rep(1,10) + rep(1,10)
library(tidyverse)
head(diamonds)
ggplot(data  = diamonds) + geom_boxplot(mapping = aes(x = cut, y = count()))
ggplot(data  = diamonds) + geom_boxplot(mapping = aes(x = cut, y = count)
)
ggplot(data  = diamonds) + geom_boxplot(mapping = aes(x = cut, y = carat))
diamond <- diamonds
mutate(diamond, countCut = count(cut))
diamonds %>% count(cut)
ggplot(data = diamond) + geom_histogram(mapping = aes(x = depth), binwidth = 0.5)
?wilcoxon.test
?wilcox.test()
diamond&depth
select(diamond, depth)
wilcox.test(select(diamond, depth))
typeof(select(diamond,depth))
x <- as.data.frame(select(diamond,depth))
typeof(x)
head(x)
wilcox.test(x)
class(as.data.frame(select(diamond, depth)))
x <- as.data.frame(select(diamond, depth))
head(x)
class(x)
typeof(x)
?shapiro.test()
shapiro.test(select(diamond, depth))
shapiro.test(as.data.frame(select(diamond,depth)))
shapiro.test(x)
is.numeric(x)
head(x)
class(x)
colnames(x)
?for
()
?for()
?map()
library(gapminder)
head(gapminder)
?round
?arrange
gapminder %>% arrange(gdpPercap)
gapminder %>% arrange(desc(gdpPercap))
gapminder %>% arrange(desc(gdpPercap), desc(pop))
gapminder %>% arrange(desc(pop))
?filter
gapminder %>% filter(country == "Netherlands")
gapminder %>% filter(country == "Netherlands") %>% arrange(year)
dutchData <- gapminder %>% filter(country == "Netherlands") %>% arrange(year)
dutchData
ggplot(data = dutchData) + geom_bar(mapping = aes(x = year, y = lifeExp))
ggplot(data = dutchData) + geom_histogram(mapping = aes(x = year, y = lifeExp))
ggplot(data = dutchData) + geom_plot(mapping = aes(x = year, y = lifeExp))
?ggplot2
ggplot(data = dutchData) + geom_line(mapping = aes(x = year, y = lifeExp))
NotFilterdutchData <- gapminder %>% filter(country == "Netherlands")
ggplot(data = NotFilterdutchData) + geom_line(mapping = aes(x = year, y = lifeExp))
Europe <- gapminder %>% filter(continent == "Europe")
ggplot(data = Europe) + geom_line(mapping = aes(x = year, y = lifeExp, colour = continent))
ggplot(data = Europe) + geom_line(mapping = aes(x = year, y = lifeExp, colour = country))
gapminder %>% filter(country == "Turkey")
gapminder %>% arrange(lifeExp)
gapminder %>% filter(year == 2007) %>% arrange(desc(lifeExp))
lifExp2007 <- gapminder %>% filter(year == 2007) %>% arrange(desc(lifeExp))
View(lifeExp2007)
View(lifExp2007)
lifExp2007 <- gapminder %>% filter(year == 2007) %>% arrange(desc(lifeExp)) %>% select(country, lifeExp)
?mutate
new <- gapminder %>% mutate(totalGdp = pop * gdpPercap)
new
new <- gapminder %>% mutate(totalGdp = pop * gdpPercap) %>% arrange(desc(totalGdp))
new
new <- gapminder %>% mutate(totalGdp = pop * gdpPercap) %>% filter(year == 2007) %>% arrange(desc(totalGdp))
new
new <- gapminder %>% mutate(totalGdp = pop * gdpPercap) %>% filter(year == 2007, continent == "Europe") %>% arrange(desc(totalGdp))
new
?signif
signif(1000,2)
signif(1000,2)/1000
library(modelr)
?bootstrap
mod <- lm(log(price) ~ log(carat), data = diamonds)
mod
summary(mod)
diamonds
diamonds %>%
mutate(lgprice = log(price), lgCarat = log(carat))
diamonds %>%
mutate(lgprice = log(price), lgCarat = log(carat)) %>%
select(lgprice, lgCarat)
diamonds %>%
mutate(lgprice = log(price), lgCarat = log(carat)) %>%
select(lgprice, lgCarat) %>%
ggplot() + geom_point(mapping = aes(x = lgprice, y = lgCarat))
tfData <- diamonds %>%
mutate(lgprice = log(price), lgCarat = log(carat)) %>%
select(lgprice, lgCarat)
tfData %>%
ggplot() + geom_point(mapping = aes(x = lgprice, y = lgCarat))
tfData
tfData %>%
lm(lgpric ~ lgCarat)
diamonds %>% lm(price ~ carat)
mod <- lm(lgprice ~ lgCarat, data = tfData)
diamonds %>% ggplot() + geom_point(mapping = aes(x = price, y = carat))
1/71
0.01408451*3600
3600/71
-1/(4*3600/71(1-40/(4*3600/71))) * log(0.99)
log(0.99)
-1/(4*3600/71(1-40/(4*3600/71)))
-1/(4*3600/71*(1-40/(4*3600/71))) * log(0.99)
1-6.172784e-05
?pnorm
qnorm(5/9,20,1)
qnorm(5/9,20,2)
qnorm(4/9,20,1)
qnorm(4/9,20,2)
?rpois
ppois(0.975,lambda = 200)
qpois(0.975,lambda = 200)
qpois(0.025,lambda = 200)
?qnorm
qnorm(5/9-20)
qnorm(5/9)
qnorm(5/9) + 20
qnorm(5/9)*sqrt(2) + 20
load("lda_user.rda")
?dist
jaccard.index(1:10, 2:20)
library(text2vec)
sim2(1:10,2:20, method = "cosine")
B = matrix(
c(2, 4, 3, 1, 5, 7),
nrow=3,
ncol=2)
C <- t(B)
sim2(B,C, method = "cosine")
x<-rnorm(10)
x<-matrix(x)
y<-matrix(rnorm(10*5),nrow=5)
x
y
install.packages("shiny")
install.packages("Shines")
install.packages("shinyjs")
install.packages("DT")
library(data.table)
data <- data.table::fread(input = "../dataset_mood_smartphone.csv",drop = 1)
generate.data <- function(N) {
# create query groups, with an average size of 25 items each
num.queries <- floor(N/25)
query <- sample(1:num.queries, N, replace=TRUE)
# X1 is a variable determined by query group only
query.level <- runif(num.queries)
X1 <- query.level[query]
# X2 varies with each item
X2 <- runif(N)
# X3 is uncorrelated with target
X3 <- runif(N)
# The target
Y <- X1 + X2
# Add some random noise to X2 that is correlated with
# queries, but uncorrelated with items
X2 <- X2 + scale(runif(num.queries))[query]
# Add some random noise to target
SNR <- 5 # signal-to-noise ratio
sigma <- sqrt(var(Y)/SNR)
Y <- Y + runif(N, 0, sigma)
data.frame(Y, query=query, X1, X2, X3)
}
cat('Generating data\n')
N=1000
data.train <- generate.data(N)
head(data.train)
data.train[order(query),]
data.train[order(data.train$query),]
setwd("~/Desktop/Assignment2")
library(data.table)
data <- data.table::fread(input = "../training_set_VU_DM_2014.csv", na.strings = c("NA", "NULL", "null"))
head(data, n=10)
missing_percentage <- apply(data, 2, function(col) sum(is.na(col))/length(col))
hist(missing_percentage)
sum(missing_percentage>=0.8)
names(missing_percentage[missing_percentage>=0.8])
keep <- names(missing_percentage[missing_percentage<0.8])
data_ver2 <- subset(data, select = keep)
num <- data_ver2[,c("prop_review_score","prop_location_score1","prop_location_score2", "prop_log_historical_price","price_usd",
"srch_length_of_stay","srch_booking_window","srch_adults_count","srch_children_count",
"srch_room_count","orig_destination_distance")]
nom <- data_ver2[,c("srch_id", "site_id", "visitor_location_country_id", "prop_country_id", "prop_id", "prop_starrating",
"prop_brand_bool", "promotion_flag", "srch_destination_id", "srch_saturday_night_bool", "random_bool",
"comp2_rate", "comp2_inv", "comp3_rate", "comp3_inv", "comp5_rate", "comp5_inv", "comp8_rate", "comp8_inv")]
myNum <- function(x){
x[is.na(x)] <- median(x, na.rm = TRUE)
x
}
myNom <- function(x){
x[is.na(x)] <- names(which.max(table(x)))
x
}
num2 <- num[, lapply(.SD, myNum)]
nom2 <- nom[, lapply(.SD, myNom)]
nom2 <- as.data.frame(lapply(nom2, function(x) as.numeric(as.character(x))))
date_time <- as.Date(data_ver2$date_time)
target <- data_ver2$booking_bool + data_ver2$click_bool
target[target==2] <- 5
hist(target)
X <- cbind(num2, nom2, target)
keep_id <- X[X$target==5]$srch_id
new_X <- subset(X, srch_id %in% keep_id)
library(xgboost)
dt <- sample(keep_id, 0.8*length(keep_id))
train <- subset(new_X, srch_id %in% dt)
test <- subset(new_X, !(srch_id %in% dt))
install.packages("gbm")
library(gbm)
colnames(train)
gbm.ndcg <- gbm(target~price_usd+prop_location_score1+prop_location_score2,          # formula
data=train,     # dataset
distribution=list(   # loss function:
name='pairwise',   # pairwise
metric="ndcg",     # ranking metric: normalized discounted cumulative gain
group='srch_id'),    # column indicating query groups
n.trees=100,        # number of trees
shrinkage=0.005,     # learning rate
interaction.depth=3, # number per splits per tree
bag.fraction = 0.5,  # subsampling fraction
train.fraction = 1,  # fraction of data for training
n.minobsinnode = 10, # minimum number of obs for split
keep.data=TRUE,      # store copy of input data in model
cv.folds=5,          # number of cross validation folds
verbose = TRUE,     # don't print progress
n.cores = 5)         # use a single core
data.train
head(train)
keep_id <- unique(X$srch_id)
X <- cbind(num2, nom2, target)
new_X <- X
keep_id <- unique(X$srch_id)
library(xgboost)
dt <- sample(keep_id, 0.8*length(keep_id))
train <- subset(new_X, srch_id %in% dt)
test <- subset(new_X, !(srch_id %in% dt))
train <- train[order(train$srch_id),]
counts <- data.frame(table(train$srch_id))
dtrain <- xgb.DMatrix(as.matrix(train[,-c(12,31)]), label = train$target, group = counts$Freq)
dtest <- xgb.DMatrix(as.matrix(test[,-c(12,31)]))
bst <- xgb.train(data = dtrain, max.depth = 5, eta = 1, nthread = 5, nround = 100, objective = "rank:pairwise",
eval_metric = "ndcg", verbose = 3)
importance_matrix <- xgb.importance(colnames(dtrain),model = bst)
print(importance_matrix)
xgb.plot.importance(importance_matrix = importance_matrix)
predictions <- predict(bst,dtest)
id <- test$srch_id
label <- test$target
source("ndcg_function.R")
metric <- ndcg(label, id, predictions, 38)
gbm.ndcg <- gbm(Y~X1+X2+X3,          # formula
data=data.train,     # dataset
distribution=list(   # loss function:
name='pairwise',   # pairwise
metric="ndcg",     # ranking metric: normalized discounted cumulative gain
group='query'),    # column indicating query groups
n.trees=2000,        # number of trees
shrinkage=0.005,     # learning rate
interaction.depth=3, # number per splits per tree
bag.fraction = 0.5,  # subsampling fraction
train.fraction = 1,  # fraction of data for training
n.minobsinnode = 10, # minimum number of obs for split
keep.data=TRUE,      # store copy of input data in model
cv.folds=5,          # number of cross validation folds
verbose = TRUE,     # don't print progress
n.cores = 5)         # use a single core
best.iter.ndcg <- gbm.perf(gbm.ndcg, method='cv')
gbm.ndcg <- gbm(target~price_usd+prop_location_score1+prop_location_score2,          # formula
data=train,     # dataset
distribution=list(   # loss function:
name='pairwise',   # pairwise
metric="ndcg",     # ranking metric: normalized discounted cumulative gain
group='srch_id'),    # column indicating query groups
n.trees=100,        # number of trees
shrinkage=0.1,     # learning rate
interaction.depth=3, # number per splits per tree
bag.fraction = 0.5,  # subsampling fraction
train.fraction = 1,  # fraction of data for training
n.minobsinnode = 10, # minimum number of obs for split
keep.data=TRUE,      # store copy of input data in model
cv.folds=5,          # number of cross validation folds
verbose = TRUE,     # don't print progress
n.cores = 7)         # use a single core
best.iter.ndcg <- gbm.perf(gbm.ndcg, method='cv')
summary(gbm.ndcg, n.trees=best.iter.ndcg, main='pairwise (ndcg)')
predictions <- predict(gbm.ndcg, test, best.iter.ndcg)
head(predictions)
predictions
ndcg5.loss=sapply(1:length(predictions), FUN=function(i) {
gbm.loss(y=test$target, predictions[[i]], w=rep(1,N), offset=NA, dist=list(name='pairwise', metric="ndcg"),
baseline=0, group=data.test$query, max.rank=38) })
ndcg5.loss=sapply(1:length(predictions), FUN=function(i) {
gbm.loss(y=test$target, predictions[[i]], w=rep(1,N), offset=NA, dist=list(name='pairwise', metric="ndcg"),
baseline=0, group=test$srch_id, max.rank=38) })
)
N <- length(predictions)
?gbm.loss
ndcg5.loss=sapply(1:length(predictions), FUN=function(i) {
gbm.loss(y=test$target, predictions[[i]], w=rep(1,N), offset=NA, dist=list(name='pairwise', metric="ndcg"),
baseline=0, group=test$srch_id, max.rank=38) })
id <- test$srch_id
label <- test$target
source("ndcg_function.R")
metric <- ndcg(label, id, predictions, 38)
metric
1-0.4696736
